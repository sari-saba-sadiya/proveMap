{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import googlemaps\n",
    "import csv\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import en_core_web_sm\n",
    "#from flashgeotext.geotext import GeoText\n",
    "#spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geotext import GeoText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from geopy.geocoders import Nominatim\n",
    "#from geopy.exc import GeocoderTimedOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import subprocess\n",
    "#import sys\n",
    "#def install(package):\n",
    "#    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install('flashgeotext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.check_call([sys.executable, \"-m\", \"spacy\",\"download\",\"en_core_web_lg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = googlemaps.Client(key=gmapKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfaId = '17'\n",
    "itemId = 'gty_'+mfaId\n",
    "tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "website = requests.get('http://www.getty.edu/art/collection/objects/'+mfaId)\n",
    "print(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(website.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearStr = []\n",
    "yearNum = []\n",
    "for date in soup.find_all(\"section\", {\"id\":\"object-section-provenance\"})[0].find_all(\"div\", {\"class\":\"small-12 medium-3 large-3 columns\"}):\n",
    "    years = re.findall(r\"[0-9]{4}\",date.text)\n",
    "    if len(years) > 0:\n",
    "        yearNum.append(int(years[-1]))\n",
    "        yearStr.append(years[-1])\n",
    "    else:\n",
    "        yearNum.append(np.nan)\n",
    "        yearStr.append('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1810, 1830, 1854, 1861, 1870, 1983]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearNum = [int((yearNum[ii-1]+yearNum[ii+1])/2) if np.isnan(y) else y for ii,y in enumerate(yearNum)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1810, 1830, 1854, 1861, 1870, 1983]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-26630db40d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpersonStr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplacesStr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"section\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"object-section-provenance\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"small-12 medium-9 large-9 columns\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcountries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "personStr = []\n",
    "placesStr = []\n",
    "for data in soup.find_all(\"section\", {\"id\":\"object-section-provenance\"})[0].find_all(\"div\", {\"class\":\"small-12 medium-9 large-9 columns\"}):\n",
    "    cities = []\n",
    "    countries = []\n",
    "    print(data.text)\n",
    "    for city in GeoText(data.text).cities:\n",
    "        cities.append(city)\n",
    "    for country in list(GeoText(data.text).country_mentions.keys()):\n",
    "        countries.append(country)\n",
    "    if len(countries) == len(cities):\n",
    "        for t in zip(cities,countries):\n",
    "            placesStr.append(' '.join(t))\n",
    "            personStr.append(data.text) \n",
    "    elif len(countries) < len(cities):\n",
    "        for city in cities:\n",
    "            placesStr.append(city)\n",
    "            personStr.append(data.text)\n",
    "    else:\n",
    "        for country in countries:\n",
    "            placesStr.append(country)\n",
    "            personStr.append(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GeoText(object):\n",
      "\n",
      "    \"\"\"Extract cities and countries from a text\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "\n",
      "    >>> places = GeoText(\"London is a great city\")\n",
      "    >>> places.cities\n",
      "    \"London\"\n",
      "\n",
      "    >>> GeoText('New York, Texas, and also China').country_mentions\n",
      "    OrderedDict([(u'US', 2), (u'CN', 1)])\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    index = build_index()\n",
      "\n",
      "    def __init__(self, text, country=None):\n",
      "        city_regex = r\"[A-ZÀ-Ú]+[a-zà-ú]+[ \\-]?(?:d[a-u].)?(?:[A-ZÀ-Ú]+[a-zà-ú]+)*\"\n",
      "        candidates = re.findall(city_regex, text)\n",
      "        # Removing white spaces from candidates\n",
      "        candidates = [candidate.strip() for candidate in candidates]\n",
      "        self.countries = [each for each in candidates\n",
      "                          if each.lower() in self.index.countries]\n",
      "        self.cities = [each for each in candidates\n",
      "                       if each.lower() in self.index.cities\n",
      "                       # country names are not considered cities\n",
      "                       and each.lower() not in self.index.countries]\n",
      "        if country is not None:\n",
      "            self.cities = [city for city in self.cities if self.index.cities[city.lower()] == country]\n",
      "\n",
      "        self.nationalities = [each for each in candidates\n",
      "                              if each.lower() in self.index.nationalities]\n",
      "\n",
      "        # Calculate number of country mentions\n",
      "        self.country_mentions = [self.index.countries[country.lower()]\n",
      "                                 for country in self.countries]\n",
      "        self.country_mentions.extend([self.index.cities[city.lower()]\n",
      "                                      for city in self.cities])\n",
      "        self.country_mentions.extend([self.index.nationalities[nationality.lower()]\n",
      "                                      for nationality in self.nationalities])\n",
      "        self.country_mentions = OrderedDict(\n",
      "            Counter(self.country_mentions).most_common())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "#inspect.getsourcefile(GeoText)\n",
    "lines = inspect.getsource(GeoText)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('FR', 1)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeoText('Paris').country_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "provhtml = soup.find_all(\"div\", {\"class\":\"detailField provenanceField\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "dischtml = soup.find_all(\"div\", {\"class\":\"detailField webDescriptionField\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(provhtml) >= 2:\n",
    "    raise ValueError(\"multiple fields detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "provStr = provhtml[0].contents[1].text # Contents[0] is the title, [1] should be the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "provStr = provStr.split('NOTE:')[0].split('\\r')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By 1990, owned by American Decorative Arts (gallery), Northampton, Massachusetts; January 24, 1990, sold by Chris Kennedy of American Decorative Arts to John Axelrod, Boston, Massachusetts; 2008, promised gift of John Axelrod to the Museum; 2014, gift of Axelrod to the MFA. (Accession date: October 29, 2014)'"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "provLst = re.split(' to ',provStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['By 1990, owned by American Decorative Arts (gallery), Northampton, Massachusetts; January 24, 1990, sold by Chris Kennedy of American Decorative Arts',\n",
       " 'John Axelrod, Boston, Massachusetts; 2008, promised gift of John Axelrod',\n",
       " 'the Museum; 2014, gift of Axelrod',\n",
       " 'the MFA. (Accession date: October 29, 2014)']"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "places = []\n",
    "addresses = []\n",
    "coordinates = []\n",
    "for element in provLst:\n",
    "    years = re.findall(r\"[0-9]{4}\", element)\n",
    "    if not years:\n",
    "        raise ValueError(\"year missing\")\n",
    "    times.append(years[-1])\n",
    "    place = list(dict.fromkeys([re.sub(r'[^\\w\\s]','',word) for word in element.split() if word[0].isupper() ]))\n",
    "    place = ' '.join(place)\n",
    "    places.append(place)\n",
    "    geocode_result = gmaps.geocode(place)\n",
    "    addresses.append(re.sub(r',',';',geocode_result[0]['formatted_address']))\n",
    "    ECo,NCo = geocode_result[0]['geometry']['location'].values()\n",
    "    coordinates.append((ECo,NCo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../provenance/'+itemId+'.csv', mode='w') as employee_file:\n",
    "    prov_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for ii,(time,place,address,coordinate) in enumerate(zip(times,places,addresses,coordinates)):\n",
    "        prov_writer.writerow([ii,time,place,address,coordinate[0],coordinate[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "internalHttp = soup.find_all(\"img\", {\"class\":\"disable-click\"})[0]['src']\n",
    "image_url = 'https://collections.mfa.org/' + internalHttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    artist = soup.find_all(\"div\", {\"class\":\"detailField peopleField\"})[0].text\n",
    "except IndexError:\n",
    "    artist = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {'disc': dischtml[0].text, \\\n",
    "            'name' : soup.find_all(\"h2\")[0].text, \\\n",
    "            'by': artist, \\\n",
    "            'image_url': image_url,\n",
    "            'yearmade' : re.findall(r\"[0-9]{4}\",soup.find_all(\"div\", {\"class\":\"detailField displayDateField\"})[0].text)[-1],\n",
    "            'tags':tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../metadata/'+itemId+'.json', 'w') as outfile:  \n",
    "    json.dump(metadata, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninternalHttp = soup.find_all(\"img\", {\"class\":\"disable-click\"})[0][\\'src\\']\\nimage_url = \\'https://collections.mfa.org/\\' + internalHttp\\nr = requests.get(image_url, stream = True)\\nr.raw.decode_content = True\\nwith open(\\'../media/\\'+itemId+\\'.jpg\\', \\'wb\\') as handler:\\n    handler.write(r.raw.read())\\n'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "internalHttp = soup.find_all(\"img\", {\"class\":\"disable-click\"})[0]['src']\n",
    "image_url = 'https://collections.mfa.org/' + internalHttp\n",
    "r = requests.get(image_url, stream = True)\n",
    "r.raw.decode_content = True\n",
    "with open('../media/'+itemId+'.jpg', 'wb') as handler:\n",
    "    handler.write(r.raw.read())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, Counter, OrderedDict\n",
    "import re\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_path(path):\n",
    "    _ROOT = '/home/sarisadiya/anaconda3/lib/python3.7/site-packages/geotext'\n",
    "    return os.path.join(_ROOT, 'data', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index():\n",
    "    \"\"\"Load information from the data directory\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A namedtuple with three fields: nationalities cities countries\n",
    "    \"\"\"\n",
    "    nationalities = read_table(get_data_path('nationalities.txt'), sep=':')\n",
    "    #print('000000000000')\n",
    "    \n",
    "    # parse http://download.geonames.org/export/dump/countryInfo.txt\n",
    "    countries = read_table(\n",
    "        get_data_path('countryInfo.txt'), usecols=[4, 0], skip=1)\n",
    "    #print('1111111111111')\n",
    "\n",
    "    # parse http://download.geonames.org/export/dump/cities15000.zip\n",
    "    cities = read_table(get_data_path('cities15000.txt'), usecols=[1, 8])\n",
    "    #print('2222222222222222')\n",
    "\n",
    "    # load and apply city patches\n",
    "    city_patches = read_table(get_data_path('citypatches.txt'))\n",
    "    cities.update(city_patches)\n",
    "\n",
    "    Index = namedtuple('Index', 'nationalities cities countries')\n",
    "    return Index(nationalities, cities, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table(filename, usecols=(0, 1), sep='\\t', comment='#', encoding='utf-8', skip=0):\n",
    "    \"\"\"Parse data files from the data directory\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: string\n",
    "        Full path to file\n",
    "\n",
    "    usecols: list, default [0, 1]\n",
    "        A list of two elements representing the columns to be parsed into a dictionary.\n",
    "        The first element will be used as keys and the second as values. Defaults to\n",
    "        the first two columns of `filename`.\n",
    "\n",
    "    sep : string, default '\\t'\n",
    "        Field delimiter.\n",
    "\n",
    "    comment : str, default '#'\n",
    "        Indicates remainder of line should not be parsed. If found at the beginning of a line,\n",
    "        the line will be ignored altogether. This parameter must be a single character.\n",
    "\n",
    "    encoding : string, default 'utf-8'\n",
    "        Encoding to use for UTF when reading/writing (ex. `utf-8`)\n",
    "\n",
    "    skip: int, default 0\n",
    "        Number of lines to skip at the beginning of the file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary with the same length as the number of lines in `filename`\n",
    "    \"\"\"\n",
    "\n",
    "    with io.open(filename, 'r', encoding=encoding) as f:\n",
    "        # skip initial lines\n",
    "        for _ in range(skip):\n",
    "            next(f)\n",
    "\n",
    "        # filter comment lines\n",
    "        lines = (line for line in f if not line.startswith(comment))\n",
    "\n",
    "        d = dict()\n",
    "        for line in lines:\n",
    "            columns = line.split(sep)\n",
    "            key = columns[usecols[0]].lower()\n",
    "            try:\n",
    "                value = columns[usecols[1]].rstrip('\\n')\n",
    "            except IndexError:\n",
    "                print(columns)\n",
    "            d[key] = value\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = read_table(get_data_path('cities15000.txt'), usecols=[1, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\tUS\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "with io.open(get_data_path('citypatches.txt'), 'r', encoding='utf-8') as f:\n",
    "    sep='\\t'\n",
    "    # skip initial lines\n",
    "    for _ in range(0):\n",
    "        next(f)\n",
    "\n",
    "    # filter comment lines\n",
    "    lines = (line for line in f if not line.startswith('#'))\n",
    "\n",
    "    d = dict()\n",
    "    for line in lines:\n",
    "        columns = line.split(sep)\n",
    "        key = columns[0].lower()\n",
    "        if key == 'paris':\n",
    "            print(line)\n",
    "            print('--------------------------')\n",
    "        value = columns[1].rstrip('\\n')\n",
    "        if key not in d.keys():\n",
    "            d[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FR'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['paris']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_regex = r\"[A-ZÀ-Ú]+[a-zà-ú]+[ \\-]?(?:d[a-u].)?(?:[A-ZÀ-Ú]+[a-zà-ú]+)*\"\n",
    "candidates = re.findall(city_regex, 'Paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [candidate.strip() for candidate in candidates]\n",
    "countries = [each for each in candidates if each.lower() in index.countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'paris' in index.countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = [each for each in candidates if each.lower() in index.countries]\n",
    "cities = [each for each in candidates if each.lower() in index.cities and each.lower() not in index.countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[each for each in candidates if each.lower() in index.nationalities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index.countries[country.lower()] for country in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index.cities[city.lower()] for city in cities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'US'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.cities['paris']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.country_mentions = [self.index.countries[country.lower()]\n",
    "                                 for country in self.countries]\n",
    "        self.country_mentions.extend([self.index.cities[city.lower()]\n",
    "                                      for city in self.cities])\n",
    "        self.country_mentions.extend([self.index.nationalities[nationality.lower()]\n",
    "                                      for nationality in self.nationalities])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
